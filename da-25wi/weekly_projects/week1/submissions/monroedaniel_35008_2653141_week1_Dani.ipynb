{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1:  EDA\n",
    "\n",
    "#### Data Analytics, Winter 2025, Weekly computational task\n",
    "\n",
    "## Description:\n",
    "We spent some time talking about exploratory data analysis in our class session on 1/7.  It's one thing to grok it abstractly, it another to do it yourself, especially in a programming language that you may not be super familiar with.\n",
    "\n",
    "We're going to start off slow.  For this week, your task is to play with *someone else's* example of exploratory data analysis.  Specifically, check out this Kaggle submission called `Intro to EDA in Python`:\n",
    "\n",
    "https://www.kaggle.com/code/imoore/intro-to-exploratory-data-analysis-eda-in-python\n",
    "\n",
    "Your task is to do the following:\n",
    "* On your own, run this  notebook from start to finish.  Kaggle has computational resources which you can use (I think), or can download the notebook and data and run them in some other environment.  Google CoLab is an easy option:  it's free, all you need is a google account, and you can upload the data directly.  However, getting setup on your own personal machine will pay dividends going forward.  \n",
    "\n",
    "* Understand the code, and take notes on the Python/Pandas syntax.  Reading and executing this code is part of your process of getting up to speed with Pandas.\n",
    "\n",
    "* Now redo at least part of the the analysis in this notebook, and expand on the analysis where you think it needs expanding.  You don't have to do everything: pick and choose.  But you should definitely add a bit of your own work.  Here are some ideas: \n",
    "    - Form a new kind of plot\n",
    "    - Generate a new statistic\n",
    "    - Add in a different data set\n",
    "    - Write a new function\n",
    "    - Trying using the `merge` function to join two pandas dataframes\n",
    "    - Steal some ideas from a more comprehensive resource such as https://github.com/jvns/pandas-cookbook\n",
    "Shoot to have a few different code blocks with a few different kinds of outputs.\n",
    "\n",
    "\n",
    "* Finish the project up by writing a Markdown paragraph *in this notebook* that summarizes your learnings.  This might be a paragraph that you submit to, say, a boss who gave you the task of becoming proficient at Pandas in one week, and at the end of that week, asks \"what did you learn\".  The paragraph should be concise, and it will be different depending on whether you are a beginner or an advanced student.\n",
    "\n",
    "When you are finished, upload your notebook to github in the `week1/submissions` folder.\n",
    "\n",
    "## Some details\n",
    "\n",
    "* **Groups:**  generally, computatiional labs will be done in groups, and I will assign the groups.  For this first week, feel free to circle up with other people, or work on your own, up to you.  \n",
    "\n",
    "* **Attribution:** In general, I only  need one notebook per group.  Indicate what role each person played, however (e.g. \"Bob typed, Mary navigated, Jen wrote the analysis\"). \n",
    "\n",
    "* **Filenames:** Make sure your filename is of the following form:  `week1_<name1>_<name2>_...<namen>.ipynb`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.service import Service as FirefoxService\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "\n",
    "# URL of the page to scrape\n",
    "URL = 'https://www.tesc.farm/produce.html'\n",
    "\n",
    "# Setup Selenium WebDriver\n",
    "options = webdriver.FirefoxOptions()\n",
    "options.add_argument('--headless')  # Run in headless mode (without GUI)\n",
    "service = FirefoxService(GeckoDriverManager().install())\n",
    "driver = webdriver.Firefox(service=service, options=options)\n",
    "\n",
    "# Load the webpage\n",
    "driver.get(URL)\n",
    "\n",
    "# Wait for the dynamic content to load\n",
    "time.sleep(3)  # Adjust sleep time as necessary, or use WebDriverWait for better control\n",
    "\n",
    "# Find the items and prices on the page\n",
    "items = driver.find_elements(By.CSS_SELECTOR, 'main#main-content div.product-grid div.product-card div.product-details h3.product-name')\n",
    "prices = driver.find_elements(By.CSS_SELECTOR, 'main#main-content div.product-grid div.product-card div.product-card-checkout p.price')\n",
    "\n",
    "# Debugging: Print the number of items and prices found\n",
    "print(f\"Number of items found: {len(items)}\")\n",
    "print(f\"Number of prices found: {len(prices)}\")\n",
    "\n",
    "# Prepare data for CSV\n",
    "data = []\n",
    "for item, price in zip(items, prices):\n",
    "    item_name = item.text.strip()\n",
    "    item_price = price.text.strip()\n",
    "    print(f\"Item: {item_name}, Price: {item_price}\")  # Debugging: Print each item and price\n",
    "    data.append([item_name, item_price])\n",
    "\n",
    "# CSV file name\n",
    "csv_file = 'veggie_scrape.csv'\n",
    "\n",
    "# Check if the file already exists\n",
    "file_exists = os.path.isfile(csv_file)\n",
    "\n",
    "# Write data to CSV\n",
    "with open(csv_file, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        # Write header if the file is newly created\n",
    "        writer.writerow(['item name', 'price'])\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"Data has been appended to {csv_file}\")\n",
    "\n",
    "# Quit the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "the above may need to be run as Py i havent tested it in jupyter, it will scrape mock vegitable data from a site another group made last quarter, this was writtern for a presentation as a take your data to flex, to show what soccer scrape did on a much smaller scale. Soccer scrape scrapes a few hundred thousand player seasons of soccer data from fbref.com i think this data would be perfect for the week we do data cleaning as many design decisions were made with the intent of \"we will clean it in pandas\" as a simmiliar design philosophy to fix it in post. I think this was generally the right call but im curious to see the consequences of my actins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('veggie_scrape.csv')\n",
    "\n",
    "# Calculate the average of the 'price' column\n",
    "average_price = df['price'].mean()\n",
    "\n",
    "# Print the average price\n",
    "print(f'The average price is: {average_price}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Your summary here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "last quarter for the software engineering class Jes split the class into 4 seperate groups most being ten people our group was the smallest with only 4 other people besides myself. Our group chose data analytics for soccer as the origional plan and so we spent three weeks doing the following tutorials before coming togahter to work on the project week 7: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.youtube.com/watch?v=aBVGKoNZQUw\n",
    "https://www.youtube.com/watch?v=yat7soj__4w\n",
    "https://www.youtube.com/watch?v=7eh4d6sabA0\n",
    "https://www.youtube.com/watch?v=XVv6mJpFOb0\n",
    "https://www.youtube.com/watch?v=r-uOLxNrNk8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our origional project scope was to build and run the code for a scraper to grab a few hundred thousand player seasons week 1 then during seccond sprint our goal was to use this data to train a model to be able to predict player performance. this was scaled back as the site we choose for its ease of scraping added bot detection the week before our first sprint, we ended up just doing the scraper part and the data is unclean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
