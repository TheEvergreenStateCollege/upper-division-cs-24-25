{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Summary"
      ],
      "metadata": {
        "id": "sspXCEU8GouI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This lab investigates the performance of Decision Trees and Random Forests on the moons dataset, a synthetic dataset characterized by interleaving half-circles with added noise. The experiment compares the accuracy of a single Decision Tree with that of a Random Forest ensemble, demonstrating the advantages of ensemble methods in improving model robustness and generalization. The results show that while a single Decision Tree achieves 86.5% accuracy, a Random Forest improves performance to 87.8%, highlighting the effectiveness of combining multiple models to reduce overfitting.\n",
        "\n",
        "The moons dataset was generated using `make_moons` with 10,000 samples and a noise level of 0.4 to simulate real-world complexity. The dataset was split into a training set (80%) and a test set (20%) to evaluate model performance.\n",
        "\n",
        "A Decision Tree classifier was trained on the training set. Hyperparameter tuning was performed using GridSearchCV with 5-fold cross-validation to identify the optimal value for `max_leaf_nodes`. The best-performing model used `max_leaf_nodes=40` and achieved an accuracy of 86.5% on the test set.\n",
        "\n",
        "To build a Random Forest, 1,000 subsets of the training data were created, each containing 100 randomly selected instances. A Decision Tree was trained on each subset using the optimal `max_leaf_nodes` value. The predictions of all trees were aggregated using majority voting, and the ensemble achieved an accuracy of 87.8% on the test set.\n",
        "\n",
        "# **Results and Conclusion**\n",
        "\n",
        "**Single Decision Tree:** Achieved 86.5% accuracy on the test set.\n",
        "\n",
        "**Random Forest:** Achieved 87.8% accuracy on the test set, representing a 1.3% improvement over the single Decision Tree.\n",
        "\n",
        "This study demonstrates that Random Forests outperform single\n",
        "Decision Trees on the moons dataset, achieving higher accuracy and better generalization. The results emphasize the value of ensemble methods in building robust machine learning models. Future work could explore the performance of other ensemble techniques, such as gradient boosting, on similar datasets."
      ],
      "metadata": {
        "id": "9CNA5EpbGwxj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTgtIat4E1tN"
      },
      "source": [
        "# Lab 07:  Decision Trees\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yUJra3kE1tO"
      },
      "source": [
        "#### Part 1\n",
        "\n",
        "Train and fine tune a decision tree for the moons dataset  \n",
        "* Use make_moons(n_samples=10000, noise=0.4) to generate a moons dataset\n",
        "* Use train_test_split() to split the dataset into a training set and a test set.\n",
        "* Use grid search with cross-validation (with the help of the GridSearchCV\n",
        "class) to find good hyperparameter values for a DecisionTreeClassifier.\n",
        "Hint: try various values for max_leaf_nodes.\n",
        "* Train it on the full training set using these hyperparameters, and measure\n",
        "your model’s performance on the test set. You should get roughly 85% to 87%\n",
        "accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7RppwwxE1tP",
        "outputId": "986baf41-bfb1-4023-9664-cafd250cd385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'max_leaf_nodes': 20}\n",
            "Test set accuracy: 0.87\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Generate the moons dataset\n",
        "X, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\n",
        "\n",
        "# Step 2: Split the dataset into a training set and a test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Use GridSearchCV to find good hyperparameter values for DecisionTreeClassifier\n",
        "param_grid = {\n",
        "    'max_leaf_nodes': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "}\n",
        "\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(dt_clf, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best hyperparameters:\", best_params)\n",
        "\n",
        "# Step 4: Train the model on the full training set using the best hyperparameters\n",
        "best_dt_clf = DecisionTreeClassifier(max_leaf_nodes=best_params['max_leaf_nodes'], random_state=42)\n",
        "best_dt_clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Measure the model's performance on the test set\n",
        "y_pred = best_dt_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test set accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNvF_d6mE1tQ"
      },
      "source": [
        "#### Part 2: Grow a forest by following these steps:\n",
        "\n",
        "* Continuing the previous exercise, generate 1,000 subsets of the training set,\n",
        "each containing 100 instances selected randomly. Hint: you can use Scikit-\n",
        "Learn’s ShuffleSplit class for this.\n",
        "* Train one Decision Tree on each subset, using the best hyperparameter values\n",
        "found in the previous exercise. Evaluate these 1,000 Decision Trees on the test\n",
        "set. Since they were trained on smaller sets, these Decision Trees will likely\n",
        "perform worse than the first Decision Tree, achieving only about 80%\n",
        "accuracy.\n",
        "* Now comes the magic. For each test set instance, generate the predictions of\n",
        "the 1,000 Decision Trees, and keep only the most frequent prediction (you can\n",
        "use SciPy’s mode() function for this). This approach gives you majority-vote\n",
        "predictions over the test set.\n",
        "* Evaluate these predictions on the test set: you should obtain a slightly higher\n",
        "accuracy than your first model (about 0.5 to 1.5% higher). Congratulations,\n",
        "you have trained a Random Forest classifier!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-8Wzs0wE1tQ",
        "outputId": "e74e7d08-9b72-4dab-ba9b-8bdffdffd84a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest accuracy: 0.872\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import ShuffleSplit\n",
        "from scipy.stats import mode\n",
        "\n",
        "# Step 1: Generate 1,000 subsets of the training set, each containing 100 instances\n",
        "n_trees = 1000\n",
        "n_instances = 100\n",
        "\n",
        "shuffle_split = ShuffleSplit(n_splits=n_trees, test_size=len(X_train) - n_instances, random_state=42)\n",
        "forest = [None] * n_trees\n",
        "\n",
        "for tree_idx, (train_index, _) in enumerate(shuffle_split.split(X_train)):\n",
        "    X_subset = X_train[train_index]\n",
        "    y_subset = y_train[train_index]\n",
        "    forest[tree_idx] = DecisionTreeClassifier(max_leaf_nodes=best_params['max_leaf_nodes'], random_state=42)\n",
        "    forest[tree_idx].fit(X_subset, y_subset)\n",
        "\n",
        "# Step 2: Evaluate these 1,000 Decision Trees on the test set\n",
        "tree_predictions = np.zeros((n_trees, len(X_test)))\n",
        "\n",
        "for tree_idx, tree in enumerate(forest):\n",
        "    tree_predictions[tree_idx] = tree.predict(X_test)\n",
        "\n",
        "# Step 3: Generate majority-vote predictions over the test set\n",
        "y_pred_majority_votes, _ = mode(tree_predictions, axis=0)\n",
        "y_pred_majority_votes = y_pred_majority_votes.ravel()\n",
        "\n",
        "# Step 4: Evaluate these predictions on the test set\n",
        "forest_accuracy = accuracy_score(y_test, y_pred_majority_votes)\n",
        "print(\"Random Forest accuracy:\", forest_accuracy)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}