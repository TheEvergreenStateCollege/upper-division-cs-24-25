# Figure 4.10
Found on page 83

![Figure 4.10](4.10.png)

This formula shows the Value Iteration, which combines the policy improvement and policy evaluation steps. Over multiple sweeps it should converge on an optimal policy valuation.

## Python Implementation

```python
# Value Iteration Function
def value_iteration(estimates={}, policy={}, discount=0.9):
  vk1 = {}
  for state in estimates:
    # Start max at lowest number possible, negative infinity
    max_value = float('-inf')
    # Loop over potential actions from the given state
    for action, outcome in policy:
      # Skip over potentials from different states
      if state not in action:
        continue
    
    # Probability of taking each action (if chosen randomly)
    probability = 1.0 / len(actions)
    # Loop over all potential actions
    for action in actions:
        # Add proportional value to the estimate
        estimate += action * discount * probability
    return estimate

# Starting Inputs:
reward = 1 # Reward from the current state
actions = [-1, 2, 5, 10] # Policy evaluation of potential actions
discount = 0.9 # Falloff over distance

estimate = get_estimate(reward, actions, discount)
print(estimate)
```

Result when executed:

```
4.6
```

The result shows the average reward of all potential actions if the action is chosen at random.
